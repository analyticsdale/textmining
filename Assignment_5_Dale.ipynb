{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#I'm not sure if all of these need to be included but I am importing them all just to be safe.\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import requests\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
     ]
    }
   ],
   "source": [
    "#Import all books into a list object. \n",
    "#Need to look into creating a function to do this.\n",
    "pathname = 'C:/Users/dmdal/OneDrive/Documents/TextMining/Books_for_Project/'\n",
    "\n",
    "pdf_Hemingway = open(pathname+'Hemingway_OldManSea.pdf','rb')     #'rb' for read binary mode\n",
    "pdfReader = PyPDF2.PdfFileReader(pdf_Hemingway)\n",
    "pages = pdfReader.numPages\n",
    "#print(type(pages))\n",
    "hemingway_text = []\n",
    "#print(type(hemingway_text))\n",
    "\n",
    "i = 1\n",
    "while i < pages: \n",
    "    pageObj_hemingway = pdfReader.getPage(i)\n",
    "    text= pageObj_hemingway.extractText()\n",
    "    text = [text]\n",
    "    #text = ''.join(row.findAll(text=True)) \n",
    "    #data = [str(text.strip())] \n",
    "    hemingway_text = hemingway_text + text\n",
    "    i = i+1\n",
    "    \n",
    "pdf_Faulkner = open(pathname+'Faulkner_As_I_Lay_Dying.pdf','rb')     #'rb' for read binary mode\n",
    "pdfReader = PyPDF2.PdfFileReader(pdf_Faulkner)\n",
    "pages = pdfReader.numPages\n",
    "#print(type(pages))\n",
    "faulkner_text = []\n",
    "#print(type(faulkner_text))\n",
    "\n",
    "i = 9\n",
    "while i < pages: \n",
    "    pageObj_faulkner = pdfReader.getPage(i)\n",
    "    text= pageObj_faulkner.extractText()\n",
    "    text = [text]\n",
    "    #text = ''.join(row.findAll(text=True)) \n",
    "    #data = [str(text.strip())] \n",
    "    faulkner_text = faulkner_text + text\n",
    "    i = i+1\n",
    "    \n",
    "pdf_Tolstoy = open(pathname+'Tolstoy_Death_Of_Ivan.pdf','rb')     #'rb' for read binary mode\n",
    "pdfReader = PyPDF2.PdfFileReader(pdf_Tolstoy)\n",
    "pages = pdfReader.numPages\n",
    "#print(type(pages))\n",
    "tolstoy_text = []\n",
    "#print(type(tolstoy_text))\n",
    "\n",
    "i = 1\n",
    "while i < pages: \n",
    "    pageObj_tolstoy = pdfReader.getPage(i)\n",
    "    text= pageObj_tolstoy.extractText()\n",
    "    text = [text]\n",
    "    #text = ''.join(row.findAll(text=True)) \n",
    "    #data = [str(text.strip())] \n",
    "    tolstoy_text = tolstoy_text + text\n",
    "    i = i+1\n",
    "    \n",
    "pdf_Hemingway2 = open(pathname+'Hemingway_TheSunAlsoRises.pdf','rb')     #'rb' for read binary mode\n",
    "pdfReader = PyPDF2.PdfFileReader(pdf_Hemingway2)\n",
    "pages = pdfReader.numPages\n",
    "#print(type(pages))\n",
    "hemingway_text2 = []\n",
    "#print(type(hemingway_text2))\n",
    "\n",
    "i = 3\n",
    "while i < pages: \n",
    "    pageObj_hemingway = pdfReader.getPage(i)\n",
    "    text= pageObj_hemingway.extractText()\n",
    "    text = [text]\n",
    "    #text = ''.join(row.findAll(text=True)) \n",
    "    #data = [str(text.strip())] \n",
    "    hemingway_text2 = hemingway_text2 + text\n",
    "    i = i+1\n",
    "    \n",
    "pdf_Faulkner2 = open(pathname+'Faulkner_light_in_august.pdf','rb')     #'rb' for read binary mode\n",
    "pdfReader = PyPDF2.PdfFileReader(pdf_Faulkner2)\n",
    "pages = pdfReader.numPages\n",
    "#print(type(pages))\n",
    "faulkner_text2 = []\n",
    "#print(type(faulkner_text2))\n",
    "\n",
    "i = 5\n",
    "while i < pages: \n",
    "    pageObj_faulkner = pdfReader.getPage(i)\n",
    "    text= pageObj_faulkner.extractText()\n",
    "    text = [text]\n",
    "    #text = ''.join(row.findAll(text=True)) \n",
    "    #data = [str(text.strip())] \n",
    "    faulkner_text2 = faulkner_text2 + text\n",
    "    i = i+1\n",
    "    \n",
    "pdf_Tolstoy2 = open(pathname+'Tolstoy_Master_and_Man.pdf','rb')     #'rb' for read binary mode\n",
    "pdfReader = PyPDF2.PdfFileReader(pdf_Tolstoy2)\n",
    "pages = pdfReader.numPages\n",
    "#print(type(pages))\n",
    "tolstoy_text2 = []\n",
    "#print(type(tolstoy_text2))\n",
    "\n",
    "i = 2\n",
    "while i < pages: \n",
    "    pageObj_tolstoy = pdfReader.getPage(i)\n",
    "    text= pageObj_tolstoy.extractText()\n",
    "    text = [text]\n",
    "    #text = ''.join(row.findAll(text=True)) \n",
    "    #data = [str(text.strip())] \n",
    "    tolstoy_text2 = tolstoy_text2 + text\n",
    "    i = i+1\n",
    "    \n",
    "pdf_Austen = open(pathname+'Austen_Persuasion.pdf','rb')     #'rb' for read binary mode\n",
    "pdfReader = PyPDF2.PdfFileReader(pdf_Austen)\n",
    "pages = pdfReader.numPages\n",
    "#print(type(pages))\n",
    "austen_text = []\n",
    "#print(type(austen_text))\n",
    "\n",
    "i = 3\n",
    "while i < pages: \n",
    "    pageObj_austen = pdfReader.getPage(i)\n",
    "    text= pageObj_austen.extractText()\n",
    "    text = [text]\n",
    "    #text = ''.join(row.findAll(text=True)) \n",
    "    #data = [str(text.strip())] \n",
    "    austen_text = austen_text + text\n",
    "    i = i+1\n",
    "    \n",
    "pdf_Austen2 = open(pathname+'Austen_Pride_Prejudice.pdf','rb')     #'rb' for read binary mode\n",
    "pdfReader = PyPDF2.PdfFileReader(pdf_Austen2)\n",
    "pages = pdfReader.numPages\n",
    "#print(type(pages))\n",
    "austen_text2 = []\n",
    "#print(type(austen_text2))\n",
    "\n",
    "i = 3\n",
    "while i < pages: \n",
    "    pageObj_austen = pdfReader.getPage(i)\n",
    "    text= pageObj_austen.extractText()\n",
    "    text = [text]\n",
    "    #text = ''.join(row.findAll(text=True)) \n",
    "    #data = [str(text.strip())] \n",
    "    austen_text2 = austen_text2 + text\n",
    "    i = i+1\n",
    "\n",
    "pdf_Tolstoy3 = open(pathname+'Tolstoy_on_Shakespeare.pdf','rb')     #'rb' for read binary mode\n",
    "pdfReader = PyPDF2.PdfFileReader(pdf_Tolstoy3)\n",
    "pages = pdfReader.numPages\n",
    "#print(type(pages))\n",
    "tolstoy_text3 = []\n",
    "#print(type(tolstoy_text3))\n",
    "\n",
    "i = 3\n",
    "while i < pages: \n",
    "    pageObj_tolstoy = pdfReader.getPage(i)\n",
    "    text= pageObj_tolstoy.extractText()\n",
    "    text = [text]\n",
    "    #text = ''.join(row.findAll(text=True)) \n",
    "    #data = [str(text.strip())] \n",
    "    tolstoy_text3 = tolstoy_text3 + text\n",
    "    i = i+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Turn each list object into a dataframe.\n",
    "hemingway_df = pd.DataFrame(hemingway_text)\n",
    "faulkner_df = pd.DataFrame(faulkner_text)\n",
    "tolstoy_df = pd.DataFrame(tolstoy_text)\n",
    "hemingway_df2 = pd.DataFrame(hemingway_text2)\n",
    "faulkner_df2 = pd.DataFrame(faulkner_text2)\n",
    "tolstoy_df2 = pd.DataFrame(tolstoy_text2)\n",
    "austen_df = pd.DataFrame(austen_text)\n",
    "austen_df2 = pd.DataFrame(austen_text2)\n",
    "tolstoy_df3 = pd.DataFrame(tolstoy_text3)\n",
    "\n",
    "#Confirm text has been brought in\n",
    "#print(hemingway_df[0:5])\n",
    "#print(faulkner_df[14])\n",
    "#print(tolstoy_df[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(hemingway_df.shape)\n",
    "#print(faulkner_df.shape)\n",
    "#print(tolstoy_df.shape)\n",
    "#print(austen_df.shape)\n",
    "#print(hemingway_df2.shape)\n",
    "#print(faulkner_df2.shape)\n",
    "#print(tolstoy_df2.shape)\n",
    "#print(austen_df2.shape)\n",
    "#print(tolstoy_df3.shape)\n",
    "\n",
    "hemingway_df['Author']= 'Hemingway'\n",
    "#print(hemingway_df.head)\n",
    "faulkner_df['Author']= 'Faulkner'\n",
    "#print(faulkner_df.head)\n",
    "tolstoy_df['Author']= 'Tolstoy'\n",
    "#print(faulkner_df.head)\n",
    "hemingway_df2['Author']= 'Hemingway'\n",
    "#print(hemingway_df.head)\n",
    "faulkner_df2['Author']= 'Faulkner'\n",
    "#print(faulkner_df.head)\n",
    "tolstoy_df2['Author']= 'Tolstoy'\n",
    "#print(faulkner_df.head)\n",
    "austen_df['Author']= 'Austen'\n",
    "#print(faulkner_df.head)\n",
    "austen_df2['Author']= 'Austen'\n",
    "#print(faulkner_df.head)\n",
    "tolstoy_df3['Author']= 'Tolstoy'\n",
    "#print(faulkner_df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Combine books\n",
    "#hemingway_df,hemingway_df2,\n",
    "books = [ faulkner_df, tolstoy_df, austen_df, faulkner_df2, tolstoy_df2, austen_df2, tolstoy_df3]\n",
    "complete_set = pd.concat(books)\n",
    "#print(complete_set.shape)\n",
    "list(complete_set)\n",
    "complete_set['OrigText'] = complete_set[0]\n",
    "#print(complete_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32mC:\\Users\\dmdal\\Anaconda3\\lib\\site-packages\\pandas\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   1944\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1945\u001b[0;31m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1946\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:4154)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:4018)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:12368)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:12322)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-fa9ced4cea34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[1;31m#print(names)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mcomplete_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\dmdal\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1995\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1997\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1999\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dmdal\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2002\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2003\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2004\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dmdal\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1348\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dmdal\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   3289\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3290\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3291\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   3292\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dmdal\\Anaconda3\\lib\\site-packages\\pandas\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   1945\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1946\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1947\u001b[0;31m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1949\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:4154)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:4018)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:12368)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:12322)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "complete_CV = CountVectorizer(binary=False, lowercase = False, stop_words = 'english') \n",
    "complete_CV_dm = complete_CV.fit_transform(complete_set['OrigText'])\n",
    "#print(complete_CV_dm.shape)\n",
    "\n",
    "names = complete_CV.get_feature_names()\n",
    "#print(type(names), len(names))\n",
    "\n",
    "count = np.sum(complete_CV_dm.toarray(), axis = 0).tolist()\n",
    "#print(type(count), len(count))\n",
    "count_df = pd.DataFrame(count, index = names, columns = ['count'])\n",
    "\n",
    "#count_df.sort_values(['count'], ascending = False).head(20)\n",
    "\n",
    "#print(names)\n",
    "complete_set[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk_stopwords = stopwords.words(\"english\")\n",
    "\n",
    "#print(type(nltk_stopwords))\n",
    "#print(len(nltk_stopwords))\n",
    "my_stopwords = nltk_stopwords + [\"br\", \"said\",\"Mr\",\"It\",\"The\",\"Mrs\",\"did\",\"But\",\"The\" \"0\", '000', '10', '100', '11', '12', '13', '14', u'15', u'16', u'17', u'18', u'19', u'20', u'25', u'30', u'40', u'50', u'500', u'60']\n",
    "#print(len(my_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12b004b26a0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgUAAAGSCAYAAABzF7JrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVyU9xQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12b0080f9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get a feel for the distribution\n",
    "complete_set['Author'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T1.  \n",
    "\n",
    "Read in or create a data frame with at least one column of text to be analyzed.  This could be the text you used previously or new text. Choose a prediction you would like to make with these data and create the appropriate feature space. Identify the labels you will be trying to predict and proceed to create a train-test split. Using default model parameters, fit 3 classifiers (decision tree, naïve bayes, and logistic regression) to your dataset and subsequently generate predictions (just like we did in class). Feel free to set a random state variable where appropriate to facilitate replication.  Assess the performance of the models using any of the measures (confusion matrices, precision, recall, f1-score, and accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# instantiate vectorizer(s)\n",
    "cv1 = CountVectorizer(lowercase=False, \n",
    "                     stop_words=my_stopwords,\n",
    "                     binary=False,\n",
    "                     max_df=0.95, \n",
    "                     min_df=0.1,\n",
    "                     ngram_range = (1,1)) \n",
    "tfidf1 = TfidfVectorizer(lowercase=False, \n",
    "                        stop_words= my_stopwords, \n",
    "                        max_df=0.95, \n",
    "                        min_df=0.1,\n",
    "                        ngram_range = (1,1)) \n",
    "\n",
    "choice = TfidfVectorizer(lowercase = False,\n",
    "                         stop_words = my_stopwords,\n",
    "                         binary=True,\n",
    "                         max_df=.92,\n",
    "                         min_df=.1,\n",
    "                         ngram_range = (1,1)) #choose your favorite parameter combination\n",
    "\n",
    "# fit and transform text\n",
    "cv_dm = cv1.fit_transform(complete_set['OrigText'])\n",
    "tfidf_dm = tfidf1.fit_transform(complete_set['OrigText'])\n",
    "choice_dm = choice.fit_transform(complete_set['OrigText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20. [cross_validation.py:44]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X = cv_dm.toarray()  #remember this is the output from the vectorizer and we are turning it into an array\n",
    "#print(type(X), X[0:10])\n",
    "\n",
    "\n",
    "y = complete_set['Author'].values #this is an array of labels\n",
    "#print(type(y), y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(complete_set['OrigText'][0:1])\n",
    "#print(\"~~~~~~~~~~~~\")\n",
    "#print(complete_set['Author'][0:1])\n",
    "#print(\"~~~~~~~~~~~~~\")\n",
    "#we created numeric representations of the text \n",
    "#print(pd.DataFrame(X,columns = cv1.get_feature_names())[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) #random_state is set seed\n",
    "\n",
    "# function creates 4 output structures - order matters\n",
    "#print(X_train)\n",
    "#print(X_train.shape)\n",
    "#print(X_test.shape)\n",
    "#print(y_train.shape)\n",
    "#print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# fit a CART model to the data\n",
    "model = DecisionTreeClassifier(random_state = 42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "clf1_expected = y_test\n",
    "clf1_predicted = model.predict(X_test)\n",
    "\n",
    "\n",
    "#print(model.score(X_test,y_test))\n",
    "\n",
    "# summarize the fit of the model\n",
    "#print(\"accuracy: \" + str(metrics.accuracy_score(clf1_expected, clf1_predicted)))\n",
    "#print(metrics.classification_report(clf1_expected, clf1_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# fit a Naive Bayes model to the data\n",
    "model = MultinomialNB()\n",
    "#print(model)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "clf2_expected = y_test\n",
    "clf2_predicted = model.predict(X_test)\n",
    "\n",
    "#print(model.score(X_test, y_test))\n",
    "\n",
    "# summarize the fit of the model\n",
    "#print(\"accuracy: \" + str(metrics.accuracy_score(clf2_expected, clf2_predicted)))\n",
    "#print(metrics.classification_report(clf2_expected, clf2_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# fit a logistic regression model to the data\n",
    "model = LogisticRegression(random_state = 42)\n",
    "#print(model)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "clf3_expected = y_test\n",
    "clf3_predicted = model.predict(X_test)\n",
    "\n",
    "#print(model.score(X_test, y_test))\n",
    "\n",
    "# summarize the fit of the model\n",
    "#print(\"accuracy: \" + str(metrics.accuracy_score(clf3_expected, clf3_predicted)))\n",
    "#print(metrics.classification_report(clf3_expected, clf3_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. \n",
    "\n",
    "Write a short description of the results of these “baseline” models. Make sure your answer is no longer than four paragraphs, and should at minimum answer these questions:\n",
    "What decisions did you make when creating your feature space? Why?  \n",
    "How do these classifiers address your question?\n",
    "How did your models perform? Are you happy with the results?  Why or why not?\n",
    "Audience: general – management or non-technical staff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A1.\n",
    "\n",
    "These \"baseline\" models preformed very well (best model is 99% and the worst model is 95%) but that is cause for concern. \n",
    "\n",
    "What decisions did you make when creating your feature space? Why? \n",
    "I brought in all of the text from the PDFs since this is just a baseline model. The words, or \"features\", I removed were words in the English stop words. I removed these words because they are words that occur frequently and don't provide much or any information. The stop words are generic/common words that do not help with our prediction and because they do not help answer our question I have decided to remove them.\n",
    "\n",
    "How do these classifiers address your question?\n",
    "I want to be able to predict an author based on the text from a page. The classifiers help me answer that question. Each classifier is an author which is what I'm trying to predict. \n",
    "\n",
    "How did your models perform? Are you happy with the results?  Why or why not?\n",
    "My models performed very well but I believe this is not a good thing. I have too many items in my word list compared to what I am trying to predict. The model is overfitted. I would need to either add more entries for prediction or break down my current entries from pages to paragraphs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T2. \n",
    "\n",
    "Using a variety of parameter settings (for classifiers or vectorizers), try to improve on the performance of the baseline models.  At least 6 separate predictions should be run and the results reported in a table.  You can use any combination of parameters and classifiers; you do not need to use all three classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X = tfidf_dm.toarray()  #remember this is the output from the vectorizer\n",
    "#print(type(X), X[0:10])\n",
    "\n",
    "\n",
    "y = complete_set['Author'].values #this is an array of labels\n",
    "#print(type(y), y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) #random_state is set seed\n",
    "\n",
    "# function creates 4 output structures - order matters\n",
    "#print(X_train.shape)\n",
    "#print(X_test.shape)\n",
    "#print(y_train.shape)\n",
    "#print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# fit a CART model to the data\n",
    "model = DecisionTreeClassifier(random_state = 42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "clf1_expected = y_test\n",
    "clf1_predicted = model.predict(X_test)\n",
    "\n",
    "\n",
    "#print(model.score(X_test,y_test))\n",
    "\n",
    "# summarize the fit of the model\n",
    "#print(\"accuracy: \" + str(metrics.accuracy_score(clf1_expected, clf1_predicted)))\n",
    "#print(metrics.classification_report(clf1_expected, clf1_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# fit a Naive Bayes model to the data\n",
    "model = MultinomialNB()\n",
    "#print(model)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "clf2_expected = y_test\n",
    "clf2_predicted = model.predict(X_test)\n",
    "\n",
    "#print(model.score(X_test, y_test))\n",
    "\n",
    "# summarize the fit of the model\n",
    "#print(\"accuracy: \" + str(metrics.accuracy_score(clf2_expected, clf2_predicted)))\n",
    "#print(metrics.classification_report(clf2_expected, clf2_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# fit a logistic regression model to the data\n",
    "model = LogisticRegression(random_state = 42)\n",
    "#print(model)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "clf3_expected = y_test\n",
    "clf3_predicted = model.predict(X_test)\n",
    "\n",
    "#print(model.score(X_test, y_test))\n",
    "\n",
    "# summarize the fit of the model\n",
    "#print(\"accuracy: \" + str(metrics.accuracy_score(clf3_expected, clf3_predicted)))\n",
    "#print(metrics.classification_report(clf3_expected, clf3_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv1 = CountVectorizer(lowercase=False, \n",
    "                     stop_words=my_stopwords,\n",
    "                     binary=False,\n",
    "                     max_df=0.9, \n",
    "                     min_df=0.1,\n",
    "                     ngram_range = (1,1))\n",
    "cv_dm = cv1.fit_transform(complete_set['OrigText'])\n",
    "X = cv_dm.toarray()  #remember this is the output from the vectorizer and we are turning it into an array\n",
    "#print(type(X), X[0:10])\n",
    "\n",
    "\n",
    "y = complete_set['Author'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# fit a CART model to the data\n",
    "model = DecisionTreeClassifier(random_state = 42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "clf1_expected = y_test\n",
    "clf1_predicted = model.predict(X_test)\n",
    "\n",
    "\n",
    "#print(model.score(X_test,y_test))\n",
    "\n",
    "# summarize the fit of the model\n",
    "#print(\"accuracy: \" + str(metrics.accuracy_score(clf1_expected, clf1_predicted)))\n",
    "#print(metrics.classification_report(clf1_expected, clf1_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# fit a Naive Bayes model to the data\n",
    "model = MultinomialNB()\n",
    "#print(model)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "clf2_expected = y_test\n",
    "clf2_predicted = model.predict(X_test)\n",
    "\n",
    "#print(model.score(X_test, y_test))\n",
    "\n",
    "# summarize the fit of the model\n",
    "#print(\"accuracy: \" + str(metrics.accuracy_score(clf2_expected, clf2_predicted)))\n",
    "#print(metrics.classification_report(clf2_expected, clf2_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# fit a logistic regression model to the data\n",
    "model = LogisticRegression(random_state = 42)\n",
    "#print(model)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "clf3_expected = y_test\n",
    "clf3_predicted = model.predict(X_test)\n",
    "\n",
    "#print(model.score(X_test, y_test))\n",
    "\n",
    "# summarize the fit of the model\n",
    "#print(\"accuracy: \" + str(metrics.accuracy_score(clf3_expected, clf3_predicted)))\n",
    "#print(metrics.classification_report(clf3_expected, clf3_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. \n",
    "\n",
    "Write a short description of the improvement you were able to make in your prediction. Make sure your answer is no longer than four paragraphs, and should at minimum answer these questions:\n",
    "What combination of classifiers and settings did you use and why?\n",
    "Which model fit “best” and what metric did you use for the comparison? Why? \n",
    "Are you happy with the results? Why or why not?  What could you do to improve on the “best” model’s performance?\n",
    "Audience: technical – fellow data scientists or other technical staff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2.\n",
    "             \n",
    "Table| Decision Tree |   Naive Bayes  |  Logistic Regression\n",
    "-----|---------------|----------------|---------------------\n",
    "Accuracy with weights|  0.9826       |     0.9548     |  0.9965\n",
    "Accuracy using 60/40 split| 0.9739        |      0.9609    |  0.9843\n",
    "\n",
    "\n",
    "What combination of classifiers and settings did you use and why?\n",
    "For the first 3 predictions I used the weight vectorizer. I used the weight vectorizers for the decision tree, naive bayes, and logistic regression. I kept the 3 classifiers I used earlier. If I removed entries for the 3rd classifier I would not have enough data. If anything, I would want to add more classifiers or at least add more data for the classifiers that I currently have. Adding more classifiers would make my model more complex and interesting. It is more interesting to predict many authors instead of 3. Adding more data for the classifiers I have now would reduce overfitting, which I believe I am currently experiencing.\n",
    "\n",
    "The next 3 predictions I used had a higher test/train split of 60% training and 40% training. The accuracy is pretty similar to the 70/30 split which shows that my model is fairly consistent. The drop in accuracy is due to the change in the min and max df. I restricted the min and max document frequency because I wanted to reduce the overall feature space. The feature space was too large and was leading to a model that was overfitted. \n",
    "\n",
    "Which model fit “best” and what metric did you use for the comparison? Why? \n",
    "The model that performed the best was my logistic regression model with the weight vectorizer. Again, I have concern that this model is overfitting, so best is a loose term. I used accuracy as my metric of comparison. I used accuracy because I am interested in overall classification. If I was working with something that was attached to revenue or cost then I would be interested in using either specificity or sensitivity. Since there is no cost associated with misclassifying a book, I am using accuracy as my metric.\n",
    "\n",
    "Are you happy with the results? Why or why not?  What could you do to improve on the “best” model’s performance?\n",
    "The results are good but I believe I need more data. I have too many features in relation to documents. To improve the model I could tinker with the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T3. \n",
    "\n",
    "Choose one preprocessing option (stemming, lemmatization, custom dictionary, custom stopwords, etc.) and recreate your feature space.  Rerun your 3 best models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv1 = CountVectorizer(lowercase=False, \n",
    "                     stop_words=my_stopwords,\n",
    "                     binary=False,\n",
    "                     max_df=0.95, \n",
    "                     min_df=0.1,\n",
    "                     ngram_range = (1,1))\n",
    "tfidf1 = TfidfVectorizer(lowercase=False, \n",
    "                        stop_words= my_stopwords, \n",
    "                        max_df=0.95, \n",
    "                        min_df=0.1,\n",
    "                        ngram_range = (1,1)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "_dict = { 'boats':'boat', 'old man':'om', 'Asianing.com':'','asiaing':'', 'fishes':'fish', '\\n':'',\n",
    "         'approaching': 'approachin', 'boating':'boat', 'wall':'walls','undergarment':'undergarments'\n",
    "        ,'tact':'tactful','wound':'wounded', 'wounding':'wounded', 'wounds':'wounded'}\n",
    "\n",
    "def multiple_replace(dict, text): \n",
    "\n",
    "  \"\"\" Replace in 'text' all occurences of any key in the given\n",
    "  dictionary by its corresponding value.  Returns the new tring.\"\"\" \n",
    "  text = str(text).lower()\n",
    "\n",
    "  # Create a regular expression  from the dictionary keys\n",
    "  regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
    "\n",
    "  # For each match, look-up corresponding value in dictionary\n",
    "  return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text)\n",
    "\n",
    "complete_set['cleantext'] = complete_set.OrigText.apply(lambda x: multiple_replace(_dict, x))\n",
    "#book_df.cleantext[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_dm = cv1.fit_transform(complete_set['cleantext'])\n",
    "tfidf_dm = tfidf1.fit_transform(complete_set['cleantext'])\n",
    "X = tfidf_dm.toarray()  \n",
    "y = complete_set['Author'].values\n",
    "\n",
    "#X = cv_dm.toarray()  \n",
    "#y = complete_set['Author'].values #this is an array of labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) #random_state is set seed\n",
    "\n",
    "# function creates 4 output structures - order matters\n",
    "#print(X_train.shape)\n",
    "#print(X_test.shape)\n",
    "#print(y_train.shape)\n",
    "#print(y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.979166666667\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state = 42)\n",
    "#print(model)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "clf3_expected = y_test\n",
    "clf3_predicted = model.predict(X_test)\n",
    "\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.954861111111\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state = 42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "clf1_expected = y_test\n",
    "clf1_predicted = model.predict(X_test)\n",
    "\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.993055555556\n"
     ]
    }
   ],
   "source": [
    "X = cv_dm.toarray()  \n",
    "y = complete_set['Author'].values #this is an array of labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "model = LogisticRegression(random_state = 42)\n",
    "#print(model)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "clf3_expected = y_test\n",
    "clf3_predicted = model.predict(X_test)\n",
    "\n",
    "print(model.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3.  \n",
    "\n",
    "Write a short description of the exercise and the outcome.  Make sure your answer is no longer than three paragraphs, and should at minimum answer these questions:\n",
    "How did the preprocessing affect the feature space? Why did you choose that option?  What changed (not just size)?\n",
    "How did the preprocessing affect the performance of the models? Is this the result you would have expected? \n",
    "Are you satisfied with the performance of any of the models?  Why or Why not?  What else might you investigate? \n",
    "Audience: technical – fellow data scientists or other technical staff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3.\n",
    "\n",
    "How did the preprocessing affect the feature space? Why did you choose that option?  What changed (not just size)?\n",
    "Preprocessing with a custom dictionary reduced my feature because it removed tokens. I chose a custom dictionary because I noticed there were several tokens that were similar and could be reduced to one token. As an example,undergarment and undergarments could be reduced down to one token since they represent the same information for predicting my question.\n",
    "\n",
    "How did the preprocessing affect the performance of the models? Is this the result you would have expected? \n",
    "It lowered my accuracy for 2 of my models slightly. I would and I wouldn't expect this. I would expect this because it is reducing tokens that could be used for prediction. One of the tokens that I removed may have held some information for prediction that I was unaware of. I wouldn't expect this because we are consolidating redundant data.I would expect the model to become more precise. I do believe the model is better off with the reduce feature space due to overfitting.\n",
    "\n",
    "Are you satisfied with the performance of any of the models?  Why or Why not?  What else might you investigate?\n",
    "Out of the 3 I am satisfied with the decision tree model because of its 95% accuracy. The 95% accuracy begins to indicate that it is not overfitted as the two models that are 99% accurate, but it is still accuracte enough to help with identifying an author based on a page that was submitted. I would also want to investigate other words to remove and add on to my current dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4.  \n",
    "\n",
    "In no more than a paragraph, outline your final project. What question are you looking to answer?  What text are you using?  With whom are you working? (Note:  you will not be strictly held to this description since there are still techniques that we will cover in class. This is meant to get people thinking about how to get started more than a week before the project is due.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A4.\n",
    "\n",
    "Team members: Cory Lowe, Nathan Shores, and Dylan Dale \n",
    "Project Outline: We would like to be able to predict a well known author based on the text from their book. The authors that are included currently for our prediction are: Hemingway, Tolstoy, Austen, and Faulkner. We are using their well known novels to assist us with our predictions. Our corpus will consist of pages from their novels. How could this be used? Library science, predict unknown authors, or predict plagiarism. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
